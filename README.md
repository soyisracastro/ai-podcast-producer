# AI Podcast Producer

An automated workflow to convert **NotebookLM** audio research into a fully produced video podcast with synchronized avatars (HeyGen) and multi-camera editing.

## ğŸš€ Overview

This toolchain solves the manual bottleneck of editing conversational AI podcasts. It takes a single audio file (generated by Google NotebookLM), identifies the speakers, splits the audio into synchronized tracks for avatar generation, and automatically assembles a final video with "TV-style" camera cuts based on who is speaking.

### Workflow
1.  **Input:** Single audio file (e.g., `m4a` from NotebookLM).
2.  **Processing (`split_audios.py`):** 
    *   Converts audio format.
    *   Performs Speaker Diarization (AI detection of who speaks when).
    *   Generates two synchronized audio tracks with silence gaps.
    *   Generates an `editing_guide.json` with timestamps.
3.  **Visual Generation (HeyGen):** 
    *   *Manual Step:* Upload tracks to HeyGen to generate avatar videos.
4.  **Assembly (`assemble_video.py`):** 
    *   Reads the JSON guide.
    *   Stitches the two videos together using multi-cam logic.
    *   Handles reaction shots (filling silence with listening gestures).

## ğŸ› ï¸ Prerequisites

## ğŸ macOS Users (M1/M2/M3)

Running AI models locally on Apple Silicon requires a specific setup (Python 3.10 + pinned dependencies). 
ğŸ‘‰ **Please read [SETUP_MAC.md](SETUP_MAC.md) for detailed installation instructions.**

*   **Python 3.8+**
*   **FFmpeg** (Required for audio processing)
    *   Mac: `brew install ffmpeg`
    *   Ubuntu: `sudo apt install ffmpeg`
    *   Windows: Install via Chocolatey or download binaries.
*   **Hugging Face Account** (For pyannote.audio model)

## ğŸ“¦ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/soyisracastro/ai-podcast-producer.git
    cd ai-podcast-producer
    ```

2.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: Create a `requirements.txt` with: `pyannote.audio`, `pydub`, `torch`, `python-dotenv`, `moviepy`)*

4.  **Environment Setup:**
    Create a `.env` file in the root directory and add your Hugging Face token:
    ```ini
    HF_TOKEN=hf_your_token_here
    ```
    *Important: You must accept the user agreement for `pyannote/speaker-diarization-3.1` ([click here](https://huggingface.co/pyannote/speaker-diarization-3.1)) and ` pyannote/segmentation-3.0` ([click here](https://huggingface.co/pyannote/segmentation-3.0)) on Hugging Face Hub.*

## ğŸ¬ Usage (Step-by-Step)

### Step 1: Audio Analysis & Splitting
Place your NotebookLM audio file in the root folder (update `INPUT_FILE` in code if needed).

```bash
python split_audios.py
```

Output:

- track_host_A.mp3
- track_host_B.mp3
- editing_guide.json

### Step 2: Video Generation (HeyGen)

- Log in to HeyGen.
- Create two videos using the generated mp3 tracks.
- Host A: Choose an avatar for the first voice.
- Host B: Choose an avatar for the second voice.
- Download the resulting videos and save them in the root folder as:
  - host_a_video.mp4
  - host_b_video.mp4

### Step 3: Automated Assembly
Run the assembler to stitch the final episode.

```bash
python assemble_video.py
```

Output:
- final_episode.mp4

## ğŸ“‚ Project Structure

```text
ai-podcast-producer/
â”œâ”€â”€ split_audios.py       # Handles diarization and audio splitting
â”œâ”€â”€ assemble_video.py     # Handles video stitching and editing logic
â”œâ”€â”€ editing_guide.json    # Generated map of cuts (Do not edit manually)
â”œâ”€â”€ .env                  # API Keys (Excluded from Git)
â”œâ”€â”€ .gitignore            # Git configuration
â””â”€â”€ README.md             # Documentation
```

## ğŸ”® Roadmap

- [x] Speaker Diarization & Audio Splitting
- [x] Automated Multi-Cam Video Assembly
- [ ] Metadata Generation: Use OpenAI/Claude API to generate Title, Description, and SEO Tags from audio transcript.
- [ ] HeyGen API Integration: Automate the video generation and download process.
- [ ] Thumbnail Automation: Generate image prompts for DALL-E 3 based on episode topics.
- [ ] YouTube Publishing: Upload final video via YouTube Data API.