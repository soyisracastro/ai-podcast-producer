# AI Podcast Producer

An automated workflow to convert **NotebookLM** audio research into a fully produced video podcast with synchronized avatars (HeyGen) and multi-camera editing.

## üöÄ Overview

This toolchain solves the manual bottleneck of editing conversational AI podcasts. It takes a single audio file (generated by Google NotebookLM), identifies the speakers, splits the audio into synchronized tracks for avatar generation, and automatically assembles a final video with "TV-style" camera cuts based on who is speaking.

### Workflow
1.  **Input:** Single audio file (e.g., `m4a` from NotebookLM).
2.  **Processing (`split_audios.py`):** 
    *   Converts audio format.
    *   Performs Speaker Diarization (AI detection of who speaks when).
    *   Generates two synchronized audio tracks with silence gaps.
    *   Generates an `editing_guide.json` with timestamps.
3.  **Visual Generation (HeyGen):** 
    *   *Manual Step:* Upload tracks to HeyGen to generate avatar videos.
4.  **Assembly (`assemble_video.py`):** 
    *   Reads the JSON guide.
    *   Stitches the two videos together using multi-cam logic.
    *   Handles reaction shots (filling silence with listening gestures).

## üõ†Ô∏è Prerequisites

## üçé macOS Users (M1/M2/M3)

Running AI models locally on Apple Silicon requires a specific setup (Python 3.10 + pinned dependencies). 
üëâ **Please read [SETUP_MAC.md](SETUP_MAC.md) for detailed installation instructions.**

*   **Python 3.8+**
*   **FFmpeg** (Required for audio processing)
    *   Mac: `brew install ffmpeg`
    *   Ubuntu: `sudo apt install ffmpeg`
    *   Windows: Install via Chocolatey or download binaries.
*   **Hugging Face Account** (For pyannote.audio model)

## üì¶ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/soyisracastro/ai-podcast-producer.git
    cd ai-podcast-producer
    ```

2.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: Create a `requirements.txt` with: `pyannote.audio`, `pydub`, `torch`, `python-dotenv`, `moviepy`)*

4.  **Environment Setup:**
    Create a `.env` file in the root directory (copy from `.env.example`):
    ```bash
    cp .env.example .env
    ```

    Edit `.env` and add your API keys:
    ```ini
    # Required for speaker diarization (split_audios.py)
    HF_TOKEN=hf_your_token_here

    # Optional - for chapter analysis (analyze_chapters.py)
    OPENAI_API_KEY=sk-your_openai_key_here
    ```

    **Important:**
    - You must accept the user agreement for `pyannote/speaker-diarization-3.1` ([click here](https://huggingface.co/pyannote/speaker-diarization-3.1)) and `pyannote/segmentation-3.0` ([click here](https://huggingface.co/pyannote/segmentation-3.0)) on Hugging Face Hub.
    - OpenAI API key is only needed if you want to use the chapter analysis feature.

## üé¨ Usage (Step-by-Step)

### Step 1: Audio Analysis & Splitting
Place your NotebookLM audio file (`.m4a` format) in the `/input` folder. The script will automatically detect it.

```bash
# The script will automatically find the .m4a file in /input
python split_audios.py
```

Output:

- track_host_A.mp3
- track_host_B.mp3
- editing_guide.json

### Step 2: Video Generation (HeyGen)

- Log in to HeyGen.
- Create two videos using the generated mp3 tracks.
- Host A: Choose an avatar for the first voice.
- Host B: Choose an avatar for the second voice.
- Download the resulting videos and save them in the root folder as:
  - host_a_video.mp4
  - host_b_video.mp4

### Step 3: Generate Subtitles (Optional)
Generate subtitles automatically from the original audio using AI transcription.

```bash
python generate_subtitles.py
```

Output:
- `{filename}.srt` (Subtitle file compatible with video editors and YouTube)

**Note:** This uses OpenAI Whisper locally (no API cost). The first run will download the AI model (~100MB for 'base' model).

### Step 3b: Analyze Chapters & Generate Metadata (Optional but Recommended)
Use AI to analyze the transcript and generate YouTube chapters, title, description, and thumbnail prompt.

```bash
python analyze_chapters.py
```

**Prerequisites:** Add `OPENAI_API_KEY` to your `.env` file (see [.env.example](.env.example))

Output:
- `{filename}_youtube.txt` - Complete YouTube metadata (title, description, chapters, thumbnail prompt)
- `{filename}_chapters.json` - Structured chapter data with timestamps
- `{filename}_clips.json` - Suggested clips for social media
- `{filename}_metadata.json` - Complete analysis data

**Cost:** ~$0.01-0.03 USD per episode (using GPT-4o-mini)

### Step 4: Automated Assembly
Run the assembler to stitch the final episode.

```bash
python assemble_video.py
```

Output:
- final_episode.mp4

### Step 5: Archive & Clean (Optional)

After completing an episode, archive the work and prepare for the next project:

```bash
# Archive with custom name
./archive_and_clean.sh "episodio_01_intro_ia"

# Or let it auto-detect the name from the .m4a file in /input
./archive_and_clean.sh
```

This will:
- Create a `.zip` file in the `/archives` directory
- Clean `/input` and `/output` directories
- Prepare the workspace for a new episode

**Optional - Upload to cloud:**
```bash
# Upload to AWS S3
./upload_to_s3.sh episodio_01_intro_ia.zip my-podcast-bucket

# Or manually copy to OneDrive/Google Drive
cp archives/episodio_01_intro_ia.zip ~/OneDrive/Podcasts/
```

üìñ For detailed archiving instructions, see [ARCHIVE_GUIDE.md](ARCHIVE_GUIDE.md)

## üìÇ Project Structure

```text
ai-podcast-producer/
‚îú‚îÄ‚îÄ split_audios.py           # Handles diarization and audio splitting
‚îú‚îÄ‚îÄ generate_subtitles.py     # Generates .srt subtitles from audio
‚îú‚îÄ‚îÄ analyze_chapters.py       # Analyzes transcript and generates YouTube metadata
‚îú‚îÄ‚îÄ assemble_video.py         # Handles video stitching and editing logic
‚îú‚îÄ‚îÄ archive_and_clean.sh      # Archive & clean input/output directories
‚îú‚îÄ‚îÄ upload_to_s3.sh           # Upload archives to AWS S3 (optional)
‚îú‚îÄ‚îÄ editing_guide.json        # Generated map of cuts (Do not edit manually)
‚îú‚îÄ‚îÄ .env                      # API Keys (Excluded from Git)
‚îú‚îÄ‚îÄ .env.example              # Template for environment variables
‚îú‚îÄ‚îÄ .gitignore                # Git configuration
‚îú‚îÄ‚îÄ README.md                 # Documentation
‚îú‚îÄ‚îÄ ARCHIVE_GUIDE.md          # Archive & backup documentation
‚îú‚îÄ‚îÄ input/                    # Input files directory
‚îú‚îÄ‚îÄ output/                   # Output files directory
‚îî‚îÄ‚îÄ archives/                 # Local backup archives (git-ignored)
```

## üîÆ Roadmap

- [x] Speaker Diarization & Audio Splitting
- [x] Automated Multi-Cam Video Assembly
- [x] Automatic Subtitle Generation (.srt)
- [x] AI Chapter Analysis & YouTube Metadata Generation (title, description, chapters, thumbnail prompt)
- [ ] HeyGen API Integration: Automate the video generation and download process.
- [ ] Clip Extraction: Automatically extract suggested clips for social media.
- [ ] YouTube Publishing: Upload final video via YouTube Data API.