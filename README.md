# AI Podcast Producer

An automated workflow to convert **NotebookLM** audio research into a fully produced video podcast with synchronized avatars (HeyGen) and multi-camera editing.

## ğŸš€ Overview

This toolchain solves the manual bottleneck of editing conversational AI podcasts. It takes a single audio file (generated by Google NotebookLM), identifies the speakers, splits the audio into synchronized tracks for avatar generation, and automatically assembles a final video with "TV-style" camera cuts based on who is speaking.

### Workflow
1.  **Input:** Single audio file (e.g., `m4a` from NotebookLM).
2.  **Processing (`split_audios.py`):** 
    *   Converts audio format.
    *   Performs Speaker Diarization (AI detection of who speaks when).
    *   Generates two synchronized audio tracks with silence gaps.
    *   Generates an `editing_guide.json` with timestamps.
3.  **Visual Generation (HeyGen):** 
    *   *Manual Step:* Upload tracks to HeyGen to generate avatar videos.
4.  **Assembly (`assemble_video.py`):** 
    *   Reads the JSON guide.
    *   Stitches the two videos together using multi-cam logic.
    *   Handles reaction shots (filling silence with listening gestures).

## ğŸ› ï¸ Prerequisites

## ğŸ macOS Users (M1/M2/M3)

Running AI models locally on Apple Silicon requires a specific setup (Python 3.10 + pinned dependencies). 
ğŸ‘‰ **Please read [SETUP_MAC.md](SETUP_MAC.md) for detailed installation instructions.**

*   **Python 3.8+**
*   **FFmpeg** (Required for audio processing)
    *   Mac: `brew install ffmpeg`
    *   Ubuntu: `sudo apt install ffmpeg`
    *   Windows: Install via Chocolatey or download binaries.
*   **Hugging Face Account** (For pyannote.audio model)

## ğŸ“¦ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/soyisracastro/ai-podcast-producer.git
    cd ai-podcast-producer
    ```

2.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: Create a `requirements.txt` with: `pyannote.audio`, `pydub`, `torch`, `python-dotenv`, `moviepy`)*

4.  **Environment Setup:**
    Create a `.env` file in the root directory and add your Hugging Face token:
    ```ini
    HF_TOKEN=hf_your_token_here
    ```
    *Important: You must accept the user agreement for `pyannote/speaker-diarization-3.1` ([click here](https://huggingface.co/pyannote/speaker-diarization-3.1)) and ` pyannote/segmentation-3.0` ([click here](https://huggingface.co/pyannote/segmentation-3.0)) on Hugging Face Hub.*

## ğŸ¬ Usage (Step-by-Step)

### Step 1: Audio Analysis & Splitting
Place your NotebookLM audio file (`.m4a` format) in the `/input` folder. The script will automatically detect it.

```bash
# The script will automatically find the .m4a file in /input
python split_audios.py
```

Output:

- track_host_A.mp3
- track_host_B.mp3
- editing_guide.json

### Step 2: Video Generation (HeyGen)

- Log in to HeyGen.
- Create two videos using the generated mp3 tracks.
- Host A: Choose an avatar for the first voice.
- Host B: Choose an avatar for the second voice.
- Download the resulting videos and save them in the root folder as:
  - host_a_video.mp4
  - host_b_video.mp4

### Step 3: Automated Assembly
Run the assembler to stitch the final episode.

```bash
python assemble_video.py
```

Output:
- final_episode.mp4

### Step 4: Archive & Clean (Optional)

After completing an episode, archive the work and prepare for the next project:

```bash
# Archive with custom name
./archive_and_clean.sh "episodio_01_intro_ia"

# Or let it auto-detect the name from the .m4a file in /input
./archive_and_clean.sh
```

This will:
- Create a `.zip` file in the `/archives` directory
- Clean `/input` and `/output` directories
- Prepare the workspace for a new episode

**Optional - Upload to cloud:**
```bash
# Upload to AWS S3
./upload_to_s3.sh episodio_01_intro_ia.zip my-podcast-bucket

# Or manually copy to OneDrive/Google Drive
cp archives/episodio_01_intro_ia.zip ~/OneDrive/Podcasts/
```

ğŸ“– For detailed archiving instructions, see [ARCHIVE_GUIDE.md](ARCHIVE_GUIDE.md)

## ğŸ“‚ Project Structure

```text
ai-podcast-producer/
â”œâ”€â”€ split_audios.py           # Handles diarization and audio splitting
â”œâ”€â”€ assemble_video.py         # Handles video stitching and editing logic
â”œâ”€â”€ archive_and_clean.sh      # Archive & clean input/output directories
â”œâ”€â”€ upload_to_s3.sh           # Upload archives to AWS S3 (optional)
â”œâ”€â”€ editing_guide.json        # Generated map of cuts (Do not edit manually)
â”œâ”€â”€ .env                      # API Keys (Excluded from Git)
â”œâ”€â”€ .gitignore                # Git configuration
â”œâ”€â”€ README.md                 # Documentation
â”œâ”€â”€ ARCHIVE_GUIDE.md          # Archive & backup documentation
â”œâ”€â”€ input/                    # Input files directory
â”œâ”€â”€ output/                   # Output files directory
â””â”€â”€ archives/                 # Local backup archives (git-ignored)
```

## ğŸ”® Roadmap

- [x] Speaker Diarization & Audio Splitting
- [x] Automated Multi-Cam Video Assembly
- [ ] Metadata Generation: Use OpenAI/Claude API to generate Title, Description, and SEO Tags from audio transcript.
- [ ] HeyGen API Integration: Automate the video generation and download process.
- [ ] Thumbnail Automation: Generate image prompts for DALL-E 3 based on episode topics.
- [ ] YouTube Publishing: Upload final video via YouTube Data API.